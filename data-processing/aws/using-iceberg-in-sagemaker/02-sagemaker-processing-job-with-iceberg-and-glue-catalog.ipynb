{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0defd9c6-dd87-4192-ba29-1bf876c99342",
   "metadata": {},
   "source": [
    "# Using Iceberg tables in SageMaker PySparkProcessor Job\n",
    "\n",
    "\n",
    "References:\n",
    "\n",
    "* https://iceberg.apache.org/docs/latest/aws/\n",
    "* https://iceberg.apache.org/docs/1.5.0/spark-writes/#overwriting-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2435bdf-be3d-4960-af79-15bf76281355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting forecasting_data_preparation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile forecasting_data_preparation.py\n",
    "import argparse\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"app inputs and outputs\")\n",
    "    parser.add_argument(\"--database_name\", type=str, help=\"name of the database in glue\")\n",
    "    parser.add_argument(\"--table_name\", type=str, help=\"name of the glue table holding iceberg data\")\n",
    "    parser.add_argument(\"--warehouse_path\", type=str, help=\"S3 URI serving as the warehouse of the iceberg table\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    catalog_name = \"glue_catalog\"\n",
    "    database_name = args.database_name\n",
    "    table_name = args.table_name\n",
    "    warehouse_path = args.warehouse_path\n",
    "\n",
    "    spark = (\n",
    "        SparkSession.builder.appName(\"PySparkIcerberg\")\n",
    "        .config(f\"spark.sql.catalog.{catalog_name}\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "        .config(f\"spark.sql.catalog.{catalog_name}.warehouse\", f\"{warehouse_path}\")\n",
    "        .config(f\"spark.sql.catalog.{catalog_name}.catalog-impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\")\n",
    "        .config(f\"spark.sql.catalog.{catalog_name}.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\n",
    "        .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "        .config(\"spark.sql.iceberg.handle-timestamp-without-timezone\", \"true\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "    # Get the list of databases from the current catalog\n",
    "    databases = spark.catalog.listDatabases()\n",
    "\n",
    "    # databases = spark.catalog.listDatabases()\n",
    "    print(\"Available Databases:\")\n",
    "    for db in databases:\n",
    "        print(f\"- {db}\")\n",
    "\n",
    "    tables = spark.catalog.listTables(database_name)\n",
    "    print(\"Available Tables:\")\n",
    "    for t in tables:\n",
    "        print(t)\n",
    "\n",
    "    # Read a table\n",
    "    df = spark.table(f\"{catalog_name}.{database_name}.{table_name}\")\n",
    "\n",
    "    print(df.head())\n",
    "\n",
    "    columns_to_keep = [\"VendorID\", \"passenger_count\", \"trip_distance\", \"fare_amount\", \"tip_amount\"]\n",
    "    cleaned_df = df.select(*columns_to_keep)\n",
    "\n",
    "    cleaned_table_name = \"cleaned_\" + table_name\n",
    "    cleaned_df.writeTo(f\"{catalog_name}.{database_name}.{cleaned_table_name}\").createOrReplace()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97c3844e-e9c2-4223-873e-675b07b4eda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!flake8 --ignore=E501 forecasting_data_preparation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d62e643-8c1a-4d33-ae2a-ea203f315415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import boto3\n",
    "import sagemaker\n",
    "from time import gmtime, strftime\n",
    "\n",
    "s3 = boto3.resource(\"s3\")\n",
    "role = sagemaker.get_execution_role()\n",
    "default_bucket = sagemaker.Session().default_bucket()\n",
    "region = sagemaker.Session().boto_region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3362deb7-b884-4d2e-9ded-3bf02b856b21",
   "metadata": {},
   "source": [
    "The classification in spark configuration must be one of the following: `['core-site', 'hadoop-env', 'hadoop-log4j', 'hive-env', 'hive-log4j', 'hive-exec-log4j', 'hive-site', 'spark-defaults', 'spark-env', 'spark-log4j', 'spark-hive-site', 'spark-metrics', 'yarn-env', 'yarn-site']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c9444c0-68c6-4cf8-9b06-abeae8ad0232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_s3(bucket, prefix, body):\n",
    "    s3_object = s3.Object(bucket, prefix)\n",
    "    s3_object.put(Body=body)\n",
    "\n",
    "\n",
    "default_spark_configuration = [\n",
    "    {\n",
    "        \"Classification\": \"spark-defaults\",\n",
    "        \"Properties\": {\n",
    "            \"spark.executor.memory\": \"2g\",\n",
    "            \"spark.executor.cores\": \"1\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Upload the raw input dataset to a unique S3 location\n",
    "prefix = \"sagemaker/parametrize-spark-config-pysparkprocessor/\"\n",
    "default_spark_conf_prefix = \"{}spark/conf/cores_1/configuration.json\".format(prefix)\n",
    "default_spark_configuration_object_s3_uri = \"s3://{}/{}\".format(\n",
    "    default_bucket, default_spark_conf_prefix\n",
    ")\n",
    "\n",
    "upload_to_s3(default_bucket, default_spark_conf_prefix, json.dumps(default_spark_configuration))\n",
    "# print(default_spark_configuration_object_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "864a3c90-f6a7-4015-82a0-9e9cb3b15bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import ParameterString\n",
    "\n",
    "spark_config_s3_uri = ParameterString(\n",
    "    name=\"SparkConfigS3Uri\",\n",
    "    default_value=default_spark_configuration_object_s3_uri,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388c7a14-27af-4470-8895-aa9d9f667ef8",
   "metadata": {},
   "source": [
    "## Data processing as pipeline step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "253df986-2c5f-426a-8a88-ea35901bb286",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "from sagemaker.workflow.pipeline_context import LocalPipelineSession\n",
    "\n",
    "local = False\n",
    "\n",
    "if local:\n",
    "    pipeline_session = LocalPipelineSession()\n",
    "    instance_count = 1\n",
    "else:\n",
    "    pipeline_session = PipelineSession()\n",
    "    instance_count = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56ee2849-38fe-47c2-a4da-98c47f512b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "from sagemaker.processing import ProcessingInput\n",
    "from sagemaker.spark.processing import _SparkProcessorBase\n",
    "\n",
    "\n",
    "pyspark_processor = PySparkProcessor(\n",
    "    base_job_name=\"sm-spark\",\n",
    "    framework_version=\"3.3\",\n",
    "    role=role,\n",
    "    instance_count=instance_count,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    max_runtime_in_seconds=1200,\n",
    "    sagemaker_session=pipeline_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d53ed1bd-3abc-421c-aa59-480b2befc716",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import ParameterString "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "193d641c-78a8-4e0e-ab3e-bd78c2cb87e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import ParameterString\n",
    "\n",
    "# input_s3_bucket_parameter = ParameterString(name=\"InputS3Bucket\", default_value=\"dataiku-migration-poc-aws\")\n",
    "# output_s3_bucket_parameter = ParameterString(name=\"OutputS3Bucket\", default_value=\"jumia-commercial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18c7bad4-cdd8-4912-b498-63c1013bac26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-07-23 17:53:25--  https://search.maven.org/remotecontent?filepath=org/apache/iceberg/iceberg-spark-runtime-3.3_2.12/1.5.2/iceberg-spark-runtime-3.3_2.12-1.5.2.jar\n",
      "Resolving search.maven.org (search.maven.org)... 35.153.115.170, 34.234.198.27\n",
      "Connecting to search.maven.org (search.maven.org)|35.153.115.170|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.3_2.12/1.5.2/iceberg-spark-runtime-3.3_2.12-1.5.2.jar [following]\n",
      "--2024-07-23 17:53:25--  https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.3_2.12/1.5.2/iceberg-spark-runtime-3.3_2.12-1.5.2.jar\n",
      "Resolving repo1.maven.org (repo1.maven.org)... 151.101.20.209, 2a04:4e42:5::209\n",
      "Connecting to repo1.maven.org (repo1.maven.org)|151.101.20.209|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 41472221 (40M) [application/java-archive]\n",
      "Saving to: ‘iceberg-spark-runtime-3.3_2.12-1.5.2.jar’\n",
      "\n",
      "iceberg-spark-runti 100%[===================>]  39.55M   211MB/s    in 0.2s    \n",
      "\n",
      "2024-07-23 17:53:26 (211 MB/s) - ‘iceberg-spark-runtime-3.3_2.12-1.5.2.jar’ saved [41472221/41472221]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://iceberg.apache.org/releases/\n",
    "!wget https://search.maven.org/remotecontent?filepath=org/apache/iceberg/iceberg-spark-runtime-3.3_2.12/1.5.2/iceberg-spark-runtime-3.3_2.12-1.5.2.jar -O iceberg-spark-runtime-3.3_2.12-1.5.2.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5636d46d-b609-4eb6-b9d3-0d381d9c0b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-07-23 17:53:26--  https://search.maven.org/remotecontent?filepath=org/apache/iceberg/iceberg-aws-bundle/1.5.2/iceberg-aws-bundle-1.5.2.jar\n",
      "Resolving search.maven.org (search.maven.org)... 34.234.198.27, 35.153.115.170\n",
      "Connecting to search.maven.org (search.maven.org)|34.234.198.27|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-aws-bundle/1.5.2/iceberg-aws-bundle-1.5.2.jar [following]\n",
      "--2024-07-23 17:53:26--  https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-aws-bundle/1.5.2/iceberg-aws-bundle-1.5.2.jar\n",
      "Resolving repo1.maven.org (repo1.maven.org)... 151.101.20.209, 2a04:4e42:5::209\n",
      "Connecting to repo1.maven.org (repo1.maven.org)|151.101.20.209|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 30512098 (29M) [application/java-archive]\n",
      "Saving to: ‘iceberg-aws-bundle-1.5.2.jar’\n",
      "\n",
      "iceberg-aws-bundle- 100%[===================>]  29.10M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2024-07-23 17:53:26 (210 MB/s) - ‘iceberg-aws-bundle-1.5.2.jar’ saved [30512098/30512098]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://search.maven.org/remotecontent?filepath=org/apache/iceberg/iceberg-aws-bundle/1.5.2/iceberg-aws-bundle-1.5.2.jar -O iceberg-aws-bundle-1.5.2.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c014cb8-ca31-45f3-9020-382361a172d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/opt/ml/processing/input/', 'conf')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark_processor._conf_container_base_path, pyspark_processor._conf_container_input_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd83c351-1076-4c5c-b946-9f8e7e2648d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:332: UserWarning: Running within a PipelineSession, there will be No Wait, No Logs, and No Job being started.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "step_args = pyspark_processor.run(\n",
    "    submit_app=\"forecasting_data_preparation.py\",\n",
    "    # submit_files=[\"pipelines/forecasting_data_preparation.sql\"],\n",
    "    submit_jars=[\"iceberg-spark-runtime-3.3_2.12-1.5.2.jar\", \"iceberg-aws-bundle-1.5.2.jar\"],\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=spark_config_s3_uri,\n",
    "            destination=f\"{pyspark_processor._conf_container_base_path}{pyspark_processor._conf_container_input_name}\",\n",
    "            input_name=_SparkProcessorBase._conf_container_input_name,\n",
    "        )\n",
    "    ],\n",
    "    arguments=[\n",
    "        \"--database_name\",\n",
    "        \"default\",\n",
    "        \"--table_name\",\n",
    "        \"taxi_dataset\",\n",
    "        \"--warehouse_path\",\n",
    "        \"s3://sagemaker-iceberg-glue-test/catalog\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94b2f821-724d-4dc5-852e-e60c2b5a10ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "spark_step_process = ProcessingStep(name=\"ForecastingDataProcessingSpark\", step_args=step_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0053a9c-6a44-4970-8a53-05ad47ba4659",
   "metadata": {},
   "source": [
    "## Setting up a SageMaker Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee256ec9-456a-4329-9cab-6df5155dcf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline_name = f\"PySparkIcebergPipeline\"\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        spark_config_s3_uri\n",
    "    ],\n",
    "    steps=[spark_step_process],\n",
    "    sagemaker_session=pipeline_session\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5d34dd-ea85-4291-8f30-38207d042ec1",
   "metadata": {},
   "source": [
    "Creating or registering the pipeline. This does not start the pipeline execution yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc7f4e8-ab0b-4cbc-86cc-f0d78280775e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b84db52-c77d-4144-86d5-580840389cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd79aa0-6062-4b5c-a753-376fd01bea2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.list_steps()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
