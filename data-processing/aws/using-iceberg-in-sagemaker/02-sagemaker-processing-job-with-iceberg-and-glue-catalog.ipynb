{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0defd9c6-dd87-4192-ba29-1bf876c99342",
   "metadata": {},
   "source": [
    "# Using Iceberg tables in SageMaker PySparkProcessor Job\n",
    "\n",
    "\n",
    "References:\n",
    "\n",
    "* https://iceberg.apache.org/docs/latest/aws/\n",
    "* https://iceberg.apache.org/docs/1.5.0/spark-writes/#overwriting-data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ee749b-0c09-4d1d-93a8-dfebec2a02b8",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0bc6ca-b3c8-4ed5-84de-596570684e58",
   "metadata": {},
   "source": [
    "Download jar dependencies, namely:abs\n",
    "* iceberg-spark-runtime-3.3_2.12-1.5.2.jar\n",
    "* iceberg-aws-bundle-1.5.2.jar -O iceberg-aws-bundle-1.5.2.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18c7bad4-cdd8-4912-b498-63c1013bac26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-07-24 17:35:53--  https://search.maven.org/remotecontent?filepath=org/apache/iceberg/iceberg-spark-runtime-3.3_2.12/1.5.2/iceberg-spark-runtime-3.3_2.12-1.5.2.jar\n",
      "Resolving search.maven.org (search.maven.org)... 34.234.198.27, 35.153.115.170\n",
      "Connecting to search.maven.org (search.maven.org)|34.234.198.27|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.3_2.12/1.5.2/iceberg-spark-runtime-3.3_2.12-1.5.2.jar [following]\n",
      "--2024-07-24 17:35:53--  https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.3_2.12/1.5.2/iceberg-spark-runtime-3.3_2.12-1.5.2.jar\n",
      "Resolving repo1.maven.org (repo1.maven.org)... 151.101.20.209, 2a04:4e42:5::209\n",
      "Connecting to repo1.maven.org (repo1.maven.org)|151.101.20.209|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 41472221 (40M) [application/java-archive]\n",
      "Saving to: ‘iceberg-spark-runtime-3.3_2.12-1.5.2.jar’\n",
      "\n",
      "iceberg-spark-runti 100%[===================>]  39.55M   226MB/s    in 0.2s    \n",
      "\n",
      "2024-07-24 17:35:53 (226 MB/s) - ‘iceberg-spark-runtime-3.3_2.12-1.5.2.jar’ saved [41472221/41472221]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://iceberg.apache.org/releases/\n",
    "!wget https://search.maven.org/remotecontent?filepath=org/apache/iceberg/iceberg-spark-runtime-3.3_2.12/1.5.2/iceberg-spark-runtime-3.3_2.12-1.5.2.jar -O iceberg-spark-runtime-3.3_2.12-1.5.2.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5636d46d-b609-4eb6-b9d3-0d381d9c0b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-07-24 17:35:53--  https://search.maven.org/remotecontent?filepath=org/apache/iceberg/iceberg-aws-bundle/1.5.2/iceberg-aws-bundle-1.5.2.jar\n",
      "Resolving search.maven.org (search.maven.org)... 35.153.115.170, 34.234.198.27\n",
      "Connecting to search.maven.org (search.maven.org)|35.153.115.170|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-aws-bundle/1.5.2/iceberg-aws-bundle-1.5.2.jar [following]\n",
      "--2024-07-24 17:35:54--  https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-aws-bundle/1.5.2/iceberg-aws-bundle-1.5.2.jar\n",
      "Resolving repo1.maven.org (repo1.maven.org)... 151.101.20.209, 2a04:4e42:5::209\n",
      "Connecting to repo1.maven.org (repo1.maven.org)|151.101.20.209|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 30512098 (29M) [application/java-archive]\n",
      "Saving to: ‘iceberg-aws-bundle-1.5.2.jar’\n",
      "\n",
      "iceberg-aws-bundle- 100%[===================>]  29.10M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2024-07-24 17:35:54 (204 MB/s) - ‘iceberg-aws-bundle-1.5.2.jar’ saved [30512098/30512098]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://search.maven.org/remotecontent?filepath=org/apache/iceberg/iceberg-aws-bundle/1.5.2/iceberg-aws-bundle-1.5.2.jar -O iceberg-aws-bundle-1.5.2.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a702cf69-12a2-452c-a2eb-0c460494ab3c",
   "metadata": {},
   "source": [
    "## Create a batch script read and write Iceberg tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2435bdf-be3d-4960-af79-15bf76281355",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting read_write_iceberg_tables.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile read_write_iceberg_tables.py\n",
    "import argparse\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"app inputs and outputs\")\n",
    "    parser.add_argument(\"--database_name\", type=str, help=\"name of the database in glue\")\n",
    "    parser.add_argument(\"--table_name\", type=str, help=\"name of the glue table holding iceberg data\")\n",
    "    parser.add_argument(\"--warehouse_path\", type=str, help=\"S3 URI serving as the warehouse of the iceberg table\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    catalog_name = \"glue_catalog\"\n",
    "    database_name = args.database_name\n",
    "    table_name = args.table_name\n",
    "    warehouse_path = args.warehouse_path\n",
    "\n",
    "    spark = (\n",
    "        SparkSession.builder.appName(\"PySparkIcerberg\")\n",
    "        .config(f\"spark.sql.catalog.{catalog_name}\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "        .config(f\"spark.sql.catalog.{catalog_name}.warehouse\", f\"{warehouse_path}\")\n",
    "        .config(f\"spark.sql.catalog.{catalog_name}.catalog-impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\")\n",
    "        .config(f\"spark.sql.catalog.{catalog_name}.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\n",
    "        .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "        .config(\"spark.sql.iceberg.handle-timestamp-without-timezone\", \"true\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "    # Get the list of databases from the current catalog\n",
    "    databases = spark.catalog.listDatabases()\n",
    "\n",
    "    # databases = spark.catalog.listDatabases()\n",
    "    print(\"Available Databases:\")\n",
    "    for db in databases:\n",
    "        print(f\"- {db}\")\n",
    "\n",
    "    tables = spark.catalog.listTables(database_name)\n",
    "    print(\"Available Tables:\")\n",
    "    for t in tables:\n",
    "        print(t)\n",
    "\n",
    "    # Read a table\n",
    "    df = spark.table(f\"{catalog_name}.{database_name}.{table_name}\")\n",
    "\n",
    "    print(df.head())\n",
    "\n",
    "    columns_to_keep = [\"VendorID\", \"passenger_count\", \"trip_distance\", \"fare_amount\", \"tip_amount\"]\n",
    "    cleaned_df = df.select(*columns_to_keep)\n",
    "\n",
    "    cleaned_table_name = \"cleaned_\" + table_name\n",
    "    cleaned_df.writeTo(f\"{catalog_name}.{database_name}.{cleaned_table_name}\").createOrReplace()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce00c34-b650-4d16-a9e9-0bd64b0dc0fd",
   "metadata": {},
   "source": [
    "## Step up a data processing Job and a SageMaker pipeline step with Iceberg and AWS Glue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d62e643-8c1a-4d33-ae2a-ea203f315415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import boto3\n",
    "import sagemaker\n",
    "from time import gmtime, strftime\n",
    "\n",
    "s3 = boto3.resource(\"s3\")\n",
    "role = sagemaker.get_execution_role()\n",
    "default_bucket = sagemaker.Session().default_bucket()\n",
    "region = sagemaker.Session().boto_region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3362deb7-b884-4d2e-9ded-3bf02b856b21",
   "metadata": {},
   "source": [
    "The classification in spark configuration must be one of the following: `['core-site', 'hadoop-env', 'hadoop-log4j', 'hive-env', 'hive-log4j', 'hive-exec-log4j', 'hive-site', 'spark-defaults', 'spark-env', 'spark-log4j', 'spark-hive-site', 'spark-metrics', 'yarn-env', 'yarn-site']` you can lookup the template of each of these configuration file on their respective official websites to identify the properties you can configure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c9444c0-68c6-4cf8-9b06-abeae8ad0232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_s3(bucket, prefix, body):\n",
    "    s3_object = s3.Object(bucket, prefix)\n",
    "    s3_object.put(Body=body)\n",
    "\n",
    "\n",
    "default_spark_configuration = [\n",
    "    {\n",
    "        \"Classification\": \"spark-defaults\",\n",
    "        \"Properties\": {\n",
    "            \"spark.executor.memory\": \"2g\",\n",
    "            \"spark.executor.cores\": \"1\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Upload the raw input dataset to a unique S3 location\n",
    "prefix = \"sagemaker/parametrize-spark-config-pysparkprocessor/\"\n",
    "default_spark_conf_prefix = \"{}spark/conf/cores_1/configuration.json\".format(prefix)\n",
    "default_spark_configuration_object_s3_uri = \"s3://{}/{}\".format(\n",
    "    default_bucket, default_spark_conf_prefix\n",
    ")\n",
    "\n",
    "upload_to_s3(default_bucket, default_spark_conf_prefix, json.dumps(default_spark_configuration))\n",
    "# print(default_spark_configuration_object_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "864a3c90-f6a7-4015-82a0-9e9cb3b15bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import ParameterString\n",
    "\n",
    "spark_config_s3_uri = ParameterString(\n",
    "    name=\"SparkConfigS3Uri\",\n",
    "    default_value=default_spark_configuration_object_s3_uri,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "253df986-2c5f-426a-8a88-ea35901bb286",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "from sagemaker.workflow.pipeline_context import LocalPipelineSession\n",
    "\n",
    "local = False\n",
    "\n",
    "if local:\n",
    "    pipeline_session = LocalPipelineSession()\n",
    "    instance_count = 1\n",
    "else:\n",
    "    pipeline_session = PipelineSession()\n",
    "    instance_count = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56ee2849-38fe-47c2-a4da-98c47f512b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "from sagemaker.processing import ProcessingInput\n",
    "from sagemaker.spark.processing import _SparkProcessorBase\n",
    "\n",
    "\n",
    "pyspark_processor = PySparkProcessor(\n",
    "    base_job_name=\"sm-spark\",\n",
    "    framework_version=\"3.3\",\n",
    "    role=role,\n",
    "    instance_count=instance_count,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    max_runtime_in_seconds=1200,\n",
    "    sagemaker_session=pipeline_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c014cb8-ca31-45f3-9020-382361a172d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/opt/ml/processing/input/', 'conf')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark_processor._conf_container_base_path, pyspark_processor._conf_container_input_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ae1d18f-d36a-4c5e-81b6-0efc4a87834d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the Glue Catalog database name, table name, and warehouse uri\n",
    "glue_database_name = \"default\"\n",
    "catalog_name = \"iceberg_catalog\"\n",
    "glue_catalog_uri = f\"s3://{default_bucket}/{catalog_name}\"\n",
    "table_name = \"taxi_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd83c351-1076-4c5c-b946-9f8e7e2648d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:332: UserWarning: Running within a PipelineSession, there will be No Wait, No Logs, and No Job being started.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "step_args = pyspark_processor.run(\n",
    "    submit_app=\"read_write_iceberg_tables.py\",\n",
    "    submit_jars=[\"iceberg-spark-runtime-3.3_2.12-1.5.2.jar\", \"iceberg-aws-bundle-1.5.2.jar\"],\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=spark_config_s3_uri,\n",
    "            destination=f\"{pyspark_processor._conf_container_base_path}{pyspark_processor._conf_container_input_name}\",\n",
    "            input_name=_SparkProcessorBase._conf_container_input_name,\n",
    "        )\n",
    "    ],\n",
    "    arguments=[\n",
    "        \"--database_name\",\n",
    "        glue_database_name,\n",
    "        \"--table_name\",\n",
    "        table_name,\n",
    "        \"--warehouse_path\",\n",
    "        glue_catalog_uri\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94b2f821-724d-4dc5-852e-e60c2b5a10ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "spark_step_process = ProcessingStep(name=\"IcebergTablesIO\", step_args=step_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0053a9c-6a44-4970-8a53-05ad47ba4659",
   "metadata": {},
   "source": [
    "## Setting up a SageMaker Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee256ec9-456a-4329-9cab-6df5155dcf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline_name = f\"PySparkIcebergPipeline\"\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        spark_config_s3_uri\n",
    "    ],\n",
    "    steps=[spark_step_process],\n",
    "    sagemaker_session=pipeline_session\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5d34dd-ea85-4291-8f30-38207d042ec1",
   "metadata": {},
   "source": [
    "Creating or registering the pipeline. This does not start the pipeline execution yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbc7f4e8-ab0b-4cbc-86cc-f0d78280775e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.spark.processing:Copying dependency from local path iceberg-spark-runtime-3.3_2.12-1.5.2.jar to tmpdir /tmp/tmpwbpwlb63\n",
      "INFO:sagemaker.spark.processing:Copying dependency from local path iceberg-aws-bundle-1.5.2.jar to tmpdir /tmp/tmpwbpwlb63\n",
      "INFO:sagemaker.spark.processing:Uploading dependencies from tmpdir /tmp/tmpwbpwlb63 to S3 s3://sagemaker-us-west-2-AWS_ACCOUNT_ID/PySparkIcebergPipeline/code/4bdb7329b49bf6df2347db0434c2223b/jars\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "INFO:sagemaker.spark.processing:Copying dependency from local path iceberg-spark-runtime-3.3_2.12-1.5.2.jar to tmpdir /tmp/tmpeim_xjuk\n",
      "INFO:sagemaker.spark.processing:Copying dependency from local path iceberg-aws-bundle-1.5.2.jar to tmpdir /tmp/tmpeim_xjuk\n",
      "INFO:sagemaker.spark.processing:Uploading dependencies from tmpdir /tmp/tmpeim_xjuk to S3 s3://sagemaker-us-west-2-AWS_ACCOUNT_ID/PySparkIcebergPipeline/code/4bdb7329b49bf6df2347db0434c2223b/jars\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-west-2:AWS_ACCOUNT_ID:pipeline/PySparkIcebergPipeline',\n",
       " 'ResponseMetadata': {'RequestId': '706ae9a3-18ab-4604-90da-e5f58e73c864',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '706ae9a3-18ab-4604-90da-e5f58e73c864',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '90',\n",
       "   'date': 'Wed, 24 Jul 2024 17:35:59 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b84db52-c77d-4144-86d5-580840389cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5dd79aa0-6062-4b5c-a753-376fd01bea2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution.list_steps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97876447-edd7-40e2-b08a-2d1b8687b539",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
